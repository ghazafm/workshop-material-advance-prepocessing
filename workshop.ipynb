{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Techniques\n",
    "> A comprehensive guide to cleaning, transforming, and optimizing your data for machine learning\n",
    "\n",
    "## Table of Contents\n",
    "- [Introduction](#introduction)\n",
    "- [Handling Missing Values](#handling-missing-values)\n",
    "- [Dealing with Outliers](#dealing-with-outliers)\n",
    "  - [Standard Deviation Method](#standard-deviation-method)\n",
    "  - [Z-Score Method](#z-score-method)\n",
    "- [Data Encoding Techniques](#data-encoding-techniques)\n",
    "  - [Label Encoding](#label-encoding)\n",
    "  - [One-Hot Encoding](#one-hot-encoding)\n",
    "  - [Ordinal Encoding](#ordinal-encoding)\n",
    "- [Advanced Preprocessing](#advanced-preprocessing)\n",
    "  - [Feature Selection](#feature-selection)\n",
    "  - [Handling Imbalanced Data](#handling-imbalanced-data)\n",
    "  - [Feature Scaling](#feature-scaling)\n",
    "  - [Data Binning](#data-binning)\n",
    "- [Summary](#summary)\n",
    "\n",
    "## Introduction\n",
    "Data preprocessing is a critical step in any data science or machine learning workflow. Clean, well-prepared data leads to better model performance, more accurate predictions, and faster development cycles. This notebook walks through essential preprocessing techniques with practical examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values\n",
    "Missing values are a common problem in real-world datasets. Let's explore how to identify and handle them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_null = {'Umur': [25, 30, None, 35, 40],\n",
    "        'Gaji': [50000, None, 60000, 65000, None],\n",
    "        'Status': ['Single', 'Single', 'Maried', None, 'Single']}\n",
    "\n",
    "df_null = pd.DataFrame(data_null)\n",
    "print(df_null.isnull().sum())\n",
    "df_null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 💡 **Insight**: The `.isnull().sum()` method quickly shows us how many missing values exist in each column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Removing Missing Values\n",
    "Simply dropping rows with missing values often removes too much data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_null.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Imputing Missing Values\n",
    "A better approach is to fill missing values with appropriate replacements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_null.fillna({'Umur': df_null['Umur'].mean(),\n",
    "           'Gaji': 0,\n",
    "           'Status': df_null['Status'].mode()[0]}, inplace=True)\n",
    "df_null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⚠️ **Warning**: Choose imputation strategies carefully based on the nature of your data and the specific column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Outliers\n",
    "Outliers can significantly impact statistical analyses and model performance. Here we'll explore methods to detect and handle them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Deviation Method\n",
    "Standard deviation measures how spread out the values are from the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Standard Deviation](https://deintrovert.wordpress.com/wp-content/uploads/2017/10/std.png?w=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std(data: list, ddof: int = 1) -> float:\n",
    "    if ddof > 1 or ddof < 0:\n",
    "        raise ValueError('ddof must be greater than 0')\n",
    "    n = len(data)\n",
    "    mean = sum(data) / n\n",
    "    total_variance = sum((x - mean) ** 2 for x in data)\n",
    "    variance = total_variance / (n - ddof)\n",
    "    stdev = variance ** 0.5\n",
    "    return stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_std = [10,1,5,1,1,1,1,2,1,1,1]\n",
    "std(data_std, ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.std(data_std, ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 📊 **Example**: We've implemented a custom standard deviation function and compared it with NumPy's implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-Score Method\n",
    "Z-score tells us how many standard deviations an element is from the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Z-Score](https://miro.medium.com/v2/resize:fit:748/0*yRjhv84-t1Xa-9pW.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_scores(data: list) -> list:\n",
    "    mean = sum(data) / len(data)\n",
    "    std_dev = std(data)\n",
    "    if std_dev == 0:\n",
    "        return [0] * len(data)\n",
    "    return [(x - mean) / std_dev for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_z = [1,1,1,1,1,1,1,1,1,1,1]\n",
    "z_scores(data_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is your score special(outlier)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_scores_1(num ,data):\n",
    "    mean = sum(data) / len(data)\n",
    "    std_dev = std(data)\n",
    "    return (num - mean) / std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nilai_teman_teman = [75, 78, 80, 82, 85]\n",
    "\n",
    "nilai_kamu = 90\n",
    "print(\"rata-rata: \", np.mean(nilai_teman_teman))\n",
    "print(\"zcore: \", z_scores_1(nilai_kamu, nilai_teman_teman))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nilai_teman_teman = [50, 60, 80, 90, 100]\n",
    "\n",
    "nilai_kamu = 90\n",
    "print(\"rata-rata: \", np.mean(nilai_teman_teman))\n",
    "print(\"zcore: \", z_scores_1(nilai_kamu, nilai_teman_teman))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 🔍 **Analysis**: A high absolute z-score suggests a value might be an outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umur_pasien = [10, 15, 5, 3, 1, 2, 2, 1,20, 21, 22, 23, 24, 3, 1, 2, 5, 8, 25, 26, 1, 9, 27, 2, 1, 1, 28, 29, 90]\n",
    "print(\"mean: \", np.mean(umur_pasien))\n",
    "print(\"median: \", np.median(umur_pasien))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores_1(90, umur_pasien)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores_1(-1, umur_pasien)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 🧪 **Experiment**: We can see how the z-score identifies the outlier value of 90 in our patient age data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Encoding Techniques\n",
    "Machine learning algorithms generally require numerical input. Encoding converts categorical data to numerical format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_le = pd.DataFrame({\n",
    "    \"Status\": [\"Married\", \"Single\", \"Married\", \"Married\", \"Single\", \"Single\", \"Irul\", \"Married\", \"Married\", \"Single\"]\n",
    "})\n",
    "data_le.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_le['Status'] = data_le['Status'].replace(\"Irul\", \"Single\")\n",
    "data_le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 🧹 **Cleaning Step**: First, we find and fix inconsistent categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding\n",
    "Transforms categories into unique integer values (0, 1, 2, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "data_le['Status_clean'] = le.fit_transform(data_le['Status'])\n",
    "# data_status.drop(columns='Status')\n",
    "data_le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 🔄 **Transformation**: Label encoding creates a single numerical column, but beware of implying ordinality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot Encoder\n",
    "Creates binary columns for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(\n",
    "    {\n",
    "    'Gender': ['Male', 'Female', 'Male', 'Female', 'Male', 'Male'],\n",
    "    }\n",
    ")\n",
    "df_pandas_encoded = pd.get_dummies(data, columns=['Gender'], dtype=int)\n",
    "df_pandas_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "data_ohe = pd.DataFrame(\n",
    "    {\n",
    "    'Gender': ['Male', 'Female', 'Male', 'Female', 'Male', \"Male\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False, dtype=int)\n",
    "encoded = ohe.fit_transform(data_ohe[['Gender']])\n",
    "\n",
    "encoded_df = pd.DataFrame(encoded, columns=ohe.get_feature_names_out(['Gender']))\n",
    "\n",
    "data_ohe = pd.concat([data_ohe, encoded_df], axis=1)\n",
    "\n",
    "data_ohe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 📋 **Best Practice**: One-hot encoding is usually preferred for nominal categories with no inherent order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal Encoding\n",
    "Used when categories have a natural order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(\n",
    "    {\n",
    "        'Education Level': ['High School', 'Bachelor', 'Master', 'PhD', 'Bachelor'],\n",
    "    }\n",
    ")\n",
    "\n",
    "education_order = {\n",
    "    'High School': 0,\n",
    "    'Bachelor': 1,\n",
    "    'Master': 2,\n",
    "    'PhD': 3\n",
    "}\n",
    "\n",
    "data['Education_OrdinalEncoded'] = data['Education Level'].map(education_order)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "data = pd.DataFrame(\n",
    "    {\n",
    "        'Education Level': ['High School', 'Bachelor', 'Master', 'PhD', 'Bachelor'],\n",
    "    }\n",
    ")\n",
    "\n",
    "order = [['High School', 'Bachelor', 'Master', 'PhD']]\n",
    "oe = OrdinalEncoder(categories=order, dtype=int)\n",
    "data['Education_OrdinalEncoded'] = oe.fit_transform(data[['Education Level']])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 🎓 **Use Case**: Ordinal encoding preserves the hierarchical relationship between categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Preprocessing\n",
    "These additional techniques can significantly improve your model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "Understanding relationships between variables is crucial for selecting the most relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "umur = np.random.normal(loc=35, scale=10, size=200)\n",
    "umur = np.clip(umur, 18, 65)\n",
    "\n",
    "gaji = 3 * (umur ** 2) + np.random.normal(0, 10000, len(umur))  \n",
    "gaji = np.clip(gaji, 3000, 150000)\n",
    "\n",
    "kesehatan_skor = 100 - (umur * 0.7) + np.random.normal(0, 7, len(umur))\n",
    "kesehatan_skor = np.clip(kesehatan_skor, 10, 100)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Umur\": umur,\n",
    "    \"Gaji\": gaji,\n",
    "    \"Kesehatan\": kesehatan_skor\n",
    "})\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pearson Correlation Coefficient\n",
    "Measures linear correlation between variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "corr_gaji, _ = pearsonr(df[\"Umur\"], df[\"Gaji\"])\n",
    "corr_kesehatan, _ = pearsonr(df[\"Umur\"], df[\"Kesehatan\"])\n",
    "\n",
    "print(f\"Pearson Correlation (Umur vs. Gaji): {corr_gaji:.2f}\")\n",
    "print(f\"Pearson Correlation (Umur vs. Kesehatan): {corr_kesehatan:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sns.regplot(x=\"Umur\", y=\"Gaji\", data=df, ax=axes[0])\n",
    "axes[0].set_title(f\"Positif Correlation (r={corr_gaji:.2f})\")\n",
    "sns.regplot(x=\"Umur\", y=\"Kesehatan\", data=df, ax=axes[1])\n",
    "axes[1].set_title(f\"Negatif Correlation (r={corr_kesehatan:.2f})\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 💯 **Interpretation**: Correlation values range from -1 to 1, with 0 indicating no correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Imbalanced Data\n",
    "Class imbalance can significantly impact model performance, especially for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=5, weights=[0.9, 0.1], random_state=42)\n",
    "\n",
    "email = pd.DataFrame(X, columns=[f\"Kata_{i}\" for i in range(1, 6)])\n",
    "email[\"Spam\"] = y\n",
    "email.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x=email['Spam'], hue=email['Spam'])\n",
    "\n",
    "plt.title(\"Email inbox\")\n",
    "plt.xlabel(\"Kelas\")\n",
    "plt.ylabel(\"Jumlah Data\")\n",
    "plt.ylim(0,1000)\n",
    "plt.xticks([0, 1], ['Bukan Spam', 'Spam'])\n",
    "plt.yticks(range(0, 1001, 100))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversampling\n",
    "Increases the number of minority class instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "undersampler = RandomOverSampler(random_state=42)\n",
    "\n",
    "X_resampled, y_resampled = undersampler.fit_resample(X, y)\n",
    "sns.countplot(x=y_resampled, hue=y_resampled)\n",
    "\n",
    "plt.title(\"Email inbox\")\n",
    "plt.xlabel(\"Kelas\")\n",
    "plt.ylabel(\"Jumlah Data\")\n",
    "plt.ylim(0,1000)\n",
    "plt.xticks([0, 1], ['Bukan Spam', 'Spam'])\n",
    "plt.yticks(range(0, 1001, 100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Undersampling\n",
    "Reduces the number of majority class instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "\n",
    "X_resampled, y_resampled = undersampler.fit_resample(X, y)\n",
    "sns.countplot(x=y_resampled, hue=y_resampled)\n",
    "\n",
    "plt.title(\"Email inbox\")\n",
    "plt.xlabel(\"Kelas\")\n",
    "plt.ylabel(\"Jumlah Data\")\n",
    "plt.ylim(0,1000)\n",
    "plt.xticks([0, 1], ['Bukan Spam', 'Spam'])\n",
    "plt.yticks(range(0, 1001, 100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⚖️ **Balance**: Both techniques help create a more balanced dataset for training models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "Scaling ensures features with different magnitudes don't dominate the model's learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "gaji = np.random.normal(10, 3, 1000)\n",
    "harga_rumah = np.random.normal(500, 150, 1000)\n",
    "data_scl = pd.DataFrame({\"Gaji (Juta IDR)\": gaji, \"Harga Rumah (Ratusan Juta IDR)\": harga_rumah})\n",
    "\n",
    "data_scl.hist(bins=30, edgecolor='black', alpha=0.7)\n",
    "plt.suptitle(\"Histogram Data Sebelum Scaling\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.kdeplot(data_scl['Gaji (Juta IDR)'], label='Gaji', color='red')\n",
    "sns.kdeplot(data_scl['Harga Rumah (Ratusan Juta IDR)'], label='Harga Rumah', color='blue')\n",
    "plt.title(\"Distribusi Data Sebelum Scaling\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_standard = StandardScaler()\n",
    "data_standard = scaler_standard.fit_transform(data_scl)\n",
    "data_standard = pd.DataFrame(data_standard, columns=data_scl.columns)\n",
    "data_standard.describe()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.kdeplot(data_standard['Gaji (Juta IDR)'], label='Gaji', color='red')\n",
    "sns.kdeplot(data_standard['Harga Rumah (Ratusan Juta IDR)'], label='Harga Rumah', color='blue')\n",
    "plt.title(\"Distribusi Data Setelah Standard Scaling\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 📏 **Note**: After standardization, both features have a mean of 0 and standard deviation of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Binning\n",
    "Binning groups continuous data into discrete categories, which can help reduce noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed, randint\n",
    "seed(42)\n",
    "age = pd.DataFrame({'age' : randint(0, 100, 100)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age['bin'] = pd.cut(age['age'], [0, 5, 17, 25, 50, 100])\n",
    "age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
    "\n",
    "# Histogram sebelum binning\n",
    "axes[0].hist(age['age'], bins=15, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xticks(np.arange(0, 110, 10))\n",
    "axes[0].grid()\n",
    "axes[0].set_title(\"Distribusi Umur Sebelum Binning\")\n",
    "axes[0].set_xlabel(\"Umur\")\n",
    "axes[0].set_ylabel(\"Frekuensi\")\n",
    "\n",
    "# Histogram setelah binning\n",
    "age['bin'].value_counts().sort_index().plot(kind='bar', ax=axes[1], color='salmon', edgecolor='black', alpha=0.7)\n",
    "axes[1].set_title(\"Distribusi Umur Setelah Binning\")\n",
    "axes[1].set_xlabel(\"Kategori Umur\")\n",
    "axes[1].set_ylabel(\"Frekuensi\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 📦 **Application**: Binning is particularly useful for creating features that capture non-linear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Proper data preprocessing is essential for successful machine learning projects. This notebook covered:\n",
    "\n",
    "1. **Handling Missing Values**: Detection and imputation strategies\n",
    "2. **Dealing with Outliers**: Standard deviation and Z-score methods\n",
    "3. **Data Encoding**: Label, one-hot, and ordinal encoding for categorical data\n",
    "4. **Advanced Techniques**:\n",
    "   - Feature selection using correlation analysis\n",
    "   - Balancing imbalanced datasets\n",
    "   - Feature scaling for normalization\n",
    "   - Binning continuous variables\n",
    "\n",
    "Remember that the choice of preprocessing techniques should be guided by your specific dataset characteristics and the requirements of your machine learning algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
